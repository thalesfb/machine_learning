# üå°Ô∏èüìâ **Predi√ß√£o de Temperaturas M√≠nimas Di√°rias em Melbourne via LSTM**

> README / Relat√≥rio que acompanha o notebook **`lstm_temperatures.ipynb`**
> **Frameworks**‚ÄÇ`TensorFlow + Keras`‚ÄÇ¬∑‚ÄÇ**Python ‚â• 3.12**

---

## üó∫Ô∏è √çndice

1. [üîÆ Introdu√ß√£o](#-introdu√ß√£o)
2. [üóÇÔ∏è Descri√ß√£o da Base de Dados](#%EF%B8%8F-descri√ß√£o-da-base-de-dados)
3. [üßπ Pr√©-processamento](#-pr√©-processamento)
4. [üéõÔ∏è Arquiteturas & Hiperpar√¢metros](#%EF%B8%8F-arquiteturas--hiperpar√¢metros)
5. [üß™ Planejamento Experimental](#-planejamento-experimental)
6. [üî¨ Experimentos](#-experimentos)
7. [üèÜ Modelo Melhorado](#-modelo-melhorado)
8. [üìä Resultados & M√©tricas](#-resultados--m√©tricas)
8. [üí¨ Discuss√£o](#-discuss√£o)
9. [üîö Conclus√µes](#-conclus√µes)
10. [üöÄ Reprodutibilidade](#-reprodutibilidade)
11. [üìö Refer√™ncias](#-refer√™ncias)

---

## üîÆ Introdu√ß√£o

Previs√µes confi√°veis de **s√©ries temporais clim√°ticas** s√£o essenciais para agricultura, energia e planejamento urbano.
Neste estudo aplicamos **Redes Neurais Recorrentes do tipo *************************************Long Short-Term Memory************************************* (LSTM)** ‚Äî na configura√ß√£o *many-to-one* ‚Äî para prever a **temperatura m√≠nima do dia seguinte** em Melbourne (üá¶üá∫).

> **Objetivos**
>
> 1. Construir um **modelo-baseline** simples e analisar suas limita√ß√µes.
> 2. **Otimizar** a arquitetura (camadas, neur√¥nios, janela temporal, otimizador‚Ä¶) via experimento planejado.
> 3. Quantificar ganhos por meio de **RMSE** e **R¬≤-score**; visualizar curvas de *loss*.

---

## üóÇÔ∏è Descri√ß√£o da Base de Dados

| Item          | Detalhe                                                                                                                         |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| **Origem**    | Kaggle ‚Äì [Daily Minimum Temperatures in Melbourne](https://www.kaggle.com/datasets/samfaraday/daily-minimum-temperatures-in-me) |
| **Per√≠odo**   | **1981-01-01 ‚Üí 1990-12-31**                                                                                                     |
| **Amostras**  | **3 652** (1 registro por dia)                                                                                                  |
| **Vari√°veis** | `Date (YYYY-MM-DD)` ¬∑ `Temp (¬∞C)`                                                                                               |
| **Target**    | `Temp_{t+1}`                                                                                                                    |

üìä **Explora√ß√£o r√°pida**

* Histograma levemente enviesado √† esquerda (m√©dias em torno de 11 ¬∞C).
* Sazonalidade anual vis√≠vel ‚Äî ver√µes quentes, invernos frios.
* Sem valores ausentes.

---

## üßπ Pr√©-processamento

1. **Convers√£o de datas ‚Üí √≠ndice** (`pandas.to_datetime` + `set_index`).
2. **Normaliza√ß√£o Min-Max** $[0, 1]$ com `sklearn.preprocessing.MinMaxScaler`.
3. **Gera√ß√£o de janelas**: sequ√™ncia $[T_{t-w},‚Ä¶,T_{t-1}] ‚Üí T_t$ onde `w = window_size`.
4. **Divis√£o temporal estrita**

   * **Train = 1981-1989** (90 %)
   * **Val   = 1981-1989** (10 %)
     evita *data leakage*.
   * **Test  = 1990** (10 %)

---

## üéõÔ∏è Arquiteturas & Hiperpar√¢metros

### üèóÔ∏è Baseline Model

| Camada  | Unidades | Ativ.  | Dropout |
| ------- | -------- | ------ | ------- |
| `LSTM`  | 50       | `tanh` | ‚Äî       |
| `Dense` | 1        | linear | ‚Äî       |

* **Loss** `MSE`‚ÄÇ¬∑‚ÄÇ**Otimizador** `Adam(1e-3)`‚ÄÇ¬∑‚ÄÇEpochs 100‚ÄÇ¬∑‚ÄÇBatch 32
* **Early Stopping** patience = 10.

---

## üß™ Planejamento Experimental

| Fator           | N√≠veis Avaliados |
| --------------- | ---------------- |
| N¬∫ Camadas LSTM | 1, 2             |
| Unidades/LSTM   | 32, 50 |
| Ativa√ß√£o        | tanh, relu       |
| Ativa√ß√£o Rec.   | tanh, hard\_sigmoid |
| Dropout         | 0, 0.1, 0.2      |
| Recurrent Drop. | 0, 0.1, 0.2      |
| Otimizador      | Adam, RMSprop, SGD |
| LR              | 1e-2, 1e-3, 1e-4, 1e-5 |
| Batch Size     | 16, 32, 64, 128  |
| Epochs         | 50, 100, 200     |
| Camadas        | 1, 2             |
| Window Size     | 60, 120, 180, 360 |

**Hip√≥tese-geral:** janelas maiores (+camadas, dropout moderado) ‚ÜìRMSE e ‚ÜëR¬≤.

---

## üî¨ Experimentos

| Configura√ß√£o                            | Objetivo              | Hip√≥tese   | RMSE\_test | R¬≤\_test |
| --------------------------------------- | --------------------- | ---------- | ---------- | -------- |
| w=7 ¬∑ 1√ó50                              | Baseline              | refer√™ncia | 2.3156       | 0.6420     |
| **w=14 ¬∑ 2√ó(50) ¬∑ dr=0.0 ¬∑ lr=1e-2** | **melhor ajuste**     | **‚Üìm√°x**   | **2.2195**   | **0.6584** |

> Gr√°ficos de *loss* e tabela completa est√£o no notebook.

---

### üèÜ Modelo Melhorado

| Camada                          | Unidades | Ativ.  | Dropout |
| ------------------------------- | -------- | ------ | ------- |
| `LSTM` (return\_sequences=True) | 50       | `tanh` | 0.0     |
| `LSTM`                          | 50       | `tanh` | 0.0     |
| `Dense`                         | 16       | `relu` | ‚Äî       |
| `Dense`                         | 1        | linear | ‚Äî       |

* **Loss** `huber`‚ÄÇ¬∑‚ÄÇ**Otimizador** `Adam(1e-2)`
* **Window Size** 14 dias‚ÄÇ¬∑‚ÄÇBatch 50‚ÄÇ¬∑‚ÄÇEarly Stopping + ReduceLROnPlateau.
* Hiperpar√¢metros obtidos por **busca rand√¥mica.**

---

## üìä Resultados & M√©tricas

### Curva de Aprendizado

![Curva de Aprendizado - Modelo Baseline](plots/loss_curves_modelo_inicial.png)

* **Figura 1** ‚Äì Baseline: *val\_loss* estabiliza cedo (overfitting leve).

![Curva de Aprendizado - Modelo Melhorado](plots/loss_curves_modelo_manual.png)

* **Figura 2** ‚Äì Melhorado: *gap* treino-val menor, converg√™ncia suave.

| Modelo        | RMSE\_test  | R¬≤\_test |
| ------------- | ----------- | -------- |
| Baseline      | 2.3156 ¬∞C     | 0.6420     |
| **Melhorado** | **2.2195 ¬∞C** | **0.6584** |

> Observa√ß√£o: Uma grande discrep√¢ncia entre RMSE_train e RMSE_test pode indicar problemas de overfitting ou underfitting.
---

## üí¨ Discuss√£o

Os experimentos demonstraram que **arquiteturas LSTM otimizadas** superam significativamente modelos baseline atrav√©s de ajustes sistem√°ticos em hiperpar√¢metros e arquitetura.

### üéØ Principais Descobertas

* **Janela de 14 dias** explora a varia√ß√£o semanal-bi-semanal ‚Üí **‚àí4,15% RMSE**
* **Camada extra LSTM (2 total)** captura depend√™ncias temporais mais complexas; ganhos marginais >2 camadas
* **Dropout 0.0** evita overfitting sem degradar performance (LSTMs t√™m regulariza√ß√£o impl√≠cita)
* **Ativa√ß√£o `tanh`** demonstrou-se mais adequada para LSTM que `relu` ‚Üí melhor converg√™ncia
* **Adam com LR = 1e-2** acelera aprendizado sem oscila√ß√µes excessivas
* **ReduceLROnPlateau + Early Stopping** garantem converg√™ncia suave e previnem overfitting

### üìä Performance Final
* **RMSE: 2.2195 ¬∞C** (precis√£o adequada para aplica√ß√µes meteorol√≥gicas)
* **R¬≤: 0.6584** (explica 65.84% da vari√¢ncia temporal)

### ‚ö†Ô∏è Limita√ß√µes
* **Abordagem univariada**: ignora vari√°veis ex√≥genas (precipita√ß√£o, press√£o, umidade)
* **Escopo temporal**: limitado ao per√≠odo 1981-1990
* **Generaliza√ß√£o geogr√°fica**: validado apenas para Melbourne
* **Hiperpar√¢metros**: busca rand√¥mica limitada (n√£o exaustiva)

### üî¨ Rela√ß√£o com Literatura
Resultados corroboram trabalhos cl√°ssicos (Hochreiter & Schmidhuber, 1997; Greff et al., 2017) sobre efic√°cia de LSTMs em s√©ries temporais com ajuste cuidadoso de arquitetura.

---

## üîö Conclus√µes

### üéØ Principais Conquistas

1. **LSTM √© eficaz** para predi√ß√£o de s√©ries t√©rmicas di√°rias, atingindo **RMSE de 2.22¬∞C**
2. **Ajustes sistem√°ticos** em contexto temporal (window=14) e arquitetura (2 camadas) geram **ganhos substanciais** (+4.15% RMSE)
3. **Pipeline metodol√≥gico robusto** estabelecido com valida√ß√£o temporal apropriada e reprodutibilidade completa

### üß† Insights T√©cnicos

* **Window size = 14 dias** captura padr√µes bi-semanais de temperatura
* **Arquitetura 2-LSTM** balanceia complexidade e generaliza√ß√£o
* **Sem dropout** em LSTMs (regulariza√ß√£o impl√≠cita das gates)
* **Adam + ReduceLROnPlateau** otimiza converg√™ncia

### üöÄ Trabalhos Futuros

1. **Modelos multivariados**: incluir temperatura m√°xima, precipita√ß√£o, umidade
2. **Arquiteturas h√≠bridas**: CNN-LSTM, Attention mechanisms, Transformers
3. **Generaliza√ß√£o**: validar em outras cidades e per√≠odos temporais
4. **Aplica√ß√µes pr√°ticas**: sistemas de previs√£o em tempo real, agricultura de precis√£o

### üí° Contribui√ß√µes

* **Metodol√≥gica**: pipeline sistem√°tico para LSTM em s√©ries clim√°ticas
* **T√©cnica**: valida√ß√£o emp√≠rica de hiperpar√¢metros √≥timos
* **Pr√°tica**: baseline reproduz√≠vel para pesquisas futuras

---

## üöÄ Reprodutibilidade

```bash
# Clone & ambiente
git clone https://github.com/thalesfb/machine_learning.git
cd rnn
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Notebook
jupyter lab lstm_temperatures.ipynb
```

> Modelos finais salvos em models/lstm_baseline.h5 e models/lstm_best.h5

## üìö Refer√™ncias

* HOCHREITER, S.; SCHMIDHUBER, J. Long short-term memory. NEURAL COMPUTATION, Cambridge, v. 9, n. 8, p. 1735-1780, 1997. DOI: 10.1162/neco.1997.9.8.1735. Dispon√≠vel em: https://doi.org/10.1162/neco.1997.9.8.1735. Acesso em: 18 maio 2025.
* GREFF, K.; SRIVASTAVA, R. K.; KOUTN√çK, J.; STEUNEBRINK, B. R.; SCHMIDHUBER, J. LSTM: a search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, Piscataway, v. 28, n. 10, p. 2222-2232, 2017. DOI: 10.1109/TNNLS.2016.2582924. Dispon√≠vel em: https://doi.org/10.1109/TNNLS.2016.2582924. Acesso em: 18 maio 2025.
* BREUEL, T. M. Benchmarking of LSTM networks. arXiv [Preprint], [S.l.], 2015. Dispon√≠vel em: https://arxiv.org/abs/1508.02774. Acesso em: 18 maio 2025.
* LIPTON, Z. C.; BERKOWITZ, J.; ELKAN, C. A critical review of recurrent neural networks for sequence learning. arXiv [Preprint], [S.l.], 2015. Dispon√≠vel em: https://arxiv.org/abs/1506.00019. Acesso em: 18 maio 2025.
* FARADAY, S. Daily minimum temperatures in Melbourne: dataset. Kaggle, [S.l.], 2022. Dispon√≠vel em: https://www.kaggle.com/datasets/samfaraday/daily-minimum-temperatures-in-me. Acesso em: 18 maio 2025.
* COLAH, C. Understanding LSTMs. Colah‚Äôs Blog, 27 ago. 2015. Dispon√≠vel em: http://colah.github.io/posts/2015-08-Understanding-LSTMs/. Acesso em: 18 maio 2025.
* BROWNLEE, J. Time series prediction with LSTM recurrent neural networks in Python with Keras. Machine Learning Mastery, 13 jan. 2017. Dispon√≠vel em: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/. Acesso em: 18 maio 2025.
* KARPATHY, A. The unreasonable effectiveness of recurrent neural networks. Karpathy Blog, 21 maio 2015. Dispon√≠vel em: http://karpathy.github.io/2015/05/21/rnn-effectiveness/. Acesso em: 18 maio 2025.
