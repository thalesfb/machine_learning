# üåü Semin√°rio ‚Äì **XGBoost**

Experimento Pr√°tico no *Wisconsin Breast Cancer Dataset*

---

## üìë Sum√°rio

1. [Motiva√ß√£o](#motiva√ß√£o)  
2. [Fundamentos Te√≥ricos](#fundamentos-te√≥ricos)  
3. [Dataset & Estratifica√ß√£o](#dataset--estratifica√ß√£o)
4. [An√°lise Explorat√≥ria (EDA)](#an√°lise-explorat√≥ria-eda)  
5. [Pr√©-processamento](#pr√©-processamento)  
6. [Particionamento Estratificado](#particionamento-estratificado)  
7. [Baseline & Tuning](#baseline--tuning)
8. [Avalia√ß√£o](#avalia√ß√£o)  
9. [Interpretabilidade](#interpretabilidade)  
10. [Reflex√£o Cr√≠tica](#reflex√£o-cr√≠tica)  
11. [Reproduzindo o Estudo](#reproduzindo-o-estudo)
12. [Artefatos](#artefatos)
13. [Refer√™ncias](#refer√™ncias)

---

## Motiva√ß√£o

> **Objetivo principal:** mostrar, na pr√°tica, todo o *pipeline* com **XGBoost**, compar√°-lo ao SVM (semin√°rio anterior) e discutir resultados, interpretabilidade e trade-offs de uso em produ√ß√£o.

---

## Fundamentos Te√≥ricos

O **XGBoost** (eXtreme Gradient Boosting) implementa *gradient boosting* com:

- **Regulariza√ß√£o L1/L2** para conter *overfitting*.
- **Paraleliza√ß√£o** por bloco + suporte a GPU.
- ***Early Stopping*** nativo.

### Fun√ß√£o objetivo

```math

\mathcal{L}(\theta)=\sum_{i=1}^{n} l\!\bigl(y_i,\hat{y}_i\bigr)+\sum_{k=1}^{K}\Omega(f_k), \\ Onde: \quad \Omega(f)=\gamma T + \tfrac{\lambda}{2}\lVert w\rVert^{2}
```

| Par√¢metro            | Papel                                              | Pontos de partida |
|----------------------|----------------------------------------------------|-------------------|
| `n_estimators`       | N¬∫ de √°rvores                                      | 200 ‚Äì 400 + `early_stopping_rounds` |
| `learning_rate`      | Contribui√ß√£o por √°rvore                            | 0.01 ‚Äì 0.1 |
| `max_depth`          | Profundidade m√°xima                                | 3 ‚Äì 6 |
| `subsample`          | % linhas por √°rvore                                | 0.7 ‚Äì 1.0 |
| `colsample_bytree`   | % colunas por √°rvore                               | 0.7 ‚Äì 1.0 |
| `gamma`              | Ganho m√≠nimo p/ split                              | 0 ‚Äì 5 |
| `lambda`, `alpha`    | Regulariza√ß√£o L2 / L1                              | 0 ‚Äì 10 |

---

## Dataset & Estratifica√ß√£o

- **Fonte:** *Breast Cancer Wisconsin* (`sklearn.datasets.load_breast_cancer`)  
- **Classes:** 0 = Maligno ¬∑ 1 = Benigno  
- **Estratifica√ß√£o** garantiu a mesma propor√ß√£o de classes em treino e teste:

| Split | Amostras | % Benigno |
|-------|----------|-----------|
| Treino| 910      | 63 % |
| Teste | 114      | 63 % |

---

## An√°lise Explorat√≥ria (EDA)

```python

import seaborn as sns, matplotlib.pyplot as plt
sns.countplot(x='target', data=df); plt.title('Distribui√ß√£o das Classes')
```

- Correla√ß√µes fortes entre `mean_*` e `worst_*`.  
- Nenhum valor ausente.  
- Algumas features altamente colineares ‚Üí regulariza√ß√£o lida bem, mas vale monitorar.

---

## Pr√©-processamento

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

> *Embora √°rvores n√£o exijam escala, mantivemos para comparabilidade com o SVM.*

---

## Particionamento Estratificado

```python
from sklearn.model_selection import StratifiedShuffleSplit
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.111, random_state=42)
```

- Propor√ß√µes conservadas (ver tabela acima).  
- `random_state` fixado ‚Üí reprodutibilidade.

---

## Baseline & Tuning

### Baseline r√°pido

```python
from xgboost import XGBClassifier
clf0 = XGBClassifier(
    n_estimators=200, max_depth=4, learning_rate=0.05,
    eval_metric='logloss', use_label_encoder=False)
clf0.fit(X_train, y_train)
```

### Tuning com `RandomizedSearchCV`

```python
param_grid = { ... }  # ver notebook
cv = StratifiedKFold(10, shuffle=True, random_state=42)
search = RandomizedSearchCV(
    XGBClassifier(eval_metric='logloss', use_label_encoder=False),
    param_grid, n_iter=50, cv=cv, scoring='roc_auc', random_state=42,
    n_jobs=-1, verbose=0)
search.fit(X_train, y_train,
           eval_set=[(X_test, y_test)],
           early_stopping_rounds=30, verbose=False)
best_clf = search.best_estimator_
```

---

## Avalia√ß√£o

```python
from sklearn.metrics import (classification_report, confusion_matrix,
                             RocCurveDisplay, PrecisionRecallDisplay)

y_pred = best_clf.predict(X_test)
print(classification_report(y_test, y_pred, digits=4))
RocCurveDisplay.from_estimator(best_clf, X_test, y_test)
PrecisionRecallDisplay.from_estimator(best_clf, X_test, y_test)
```

| M√©trica  | Valor |
|----------|-------|
| AUC-ROC  | **0.99** |
| Precis√£o | 0.96 |
| Recall   | 0.95 |
| F1-score | 0.95 |

Confusion Matrix: 2 FP ¬∑ 3 FN.

---

## Interpretabilidade

### Import√¢ncia de Features nativa

```python
from xgboost import plot_importance
plot_importance(best_clf, max_num_features=10)
```

### SHAP

```python
import shap
explainer = shap.TreeExplainer(best_clf)
shap_vals = explainer.shap_values(X_test)
shap.summary_plot(shap_vals, X_test, plot_type="dot")
```

- `worst_area`, `worst_radius` e `worst_concave_points` dominam a contribui√ß√£o.  
- Valores altos dessas vari√°veis aumentam probabilidade de maligno.

---

## Reflex√£o Cr√≠tica

| Quest√£o                           | Insight |
|----------------------------------|---------|
| **Overfitting**                  | N√£o foi observado: *early stopping* + regulariza√ß√£o mantiveram gap < 1 pp entre treino e valida√ß√£o. |
| **Compara√ß√£o com SVM (semin√°rio)** | XGBoost ‚Üì AUC (-0.0264), ‚Üì tempo de predi√ß√£o (~4√ó). |
| **Limita√ß√µes**                   | Tuning extenso; riscos de *data leakage* se *pipeline* n√£o for bem fechado. |
| **Pr√≥ximos Passos**              | LightGBM / CatBoost; *stacking* com SVM; calibra√ß√£o de probabilidades (`CalibratedClassifierCV`). |

---

## Reproduzindo o Estudo

```bash
# 1 ¬∑ Clone o reposit√≥rio
git clone https://github.com/thalesfb/machine_learning.git
cd machine_learning/seminar/xgboost

# 2 ¬∑ Ambiente
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 3 ¬∑ Execute
jupyter lab xgboost.ipynb
```

> **Vers√£o Python ‚â• 3.10** recomendada.

---

## Artefatos

| Arquivo | Prop√≥sito |
|---------|-----------|
| [**`xgboost.ipynb`**](./xgboost.ipynb) | Notebook completo (c√≥digo + gr√°ficos). |
| [**`slides`**](https://docs.google.com/presentation/d/1CKbIO8EjqNqdgZhZIB3_YTu7wHhXNLksIFju-FgbMLk/edit#slide=id.p) | Apresenta√ß√£o de 15 min. |
| [**`requirements.txt`**](./requirements.txt) | Lista de depend√™ncias (‚âà 80 MB instala√ß√£o clean). |
| **Dataset** | Carregado diretamente via *scikit-learn* (sem download manual). |

---

## Refer√™ncias

- üîó[XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)
- üîó[Breast Cancer Wisconsin Dataset - Kaggle](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)
- üîó[XGBoost Docs](https://xgboost.readthedocs.io/)
- üîó[StatQuest: Gradient Boosting Explained](https://youtu.be/3CC4N4z3GJc)
- üîó[Kaggle XGBoost Tutorials](https://www.kaggle.com/tag/xgboost)
- üîó[Scikit-learn Docs](https://scikit-learn.org/stable/index.html)
- üîó[Bias-Variance Tradeoff](https://scott.fortmann-roe.com/docs/BiasVariance.html)

---

> *‚ÄúAll models are wrong, but some are useful.‚Äù ‚Äì George Box*  
> **Fa√ßa a valida√ß√£o cruzada sempre.**
