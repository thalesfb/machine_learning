# ğŸŒŸ SeminÃ¡rio â€“ **XGBoost** do Zero ao AvanÃ§ado  
<sub>Experimento prÃ¡tico no Wisconsin Breast Cancer Dataset</sub>

---

## ğŸ“˜ RecomendaÃ§Ãµes RÃ¡pidas

| âœ”ï¸ Como usar | ğŸ’¡ Dicas extras |
|-------------|----------------|
| Execute as cÃ©lulas do notebook na ordem. | Altere hiperparÃ¢metros e observe o impacto nas mÃ©tricas. |
| Mantenha um conjunto de validaÃ§Ã£o (`early_stopping_rounds`). | Compare com SVM ou Random Forest para sentir diferenÃ§as. |
| Explore `scale_pos_weight` se as classes estiverem desbalanceadas. | Use SHAP para explicar prediÃ§Ãµes individuais. |

---

## ğŸ¯ Objetivos

1. **Entender, na teoria, como o XGBoost funciona** (boosting, regularizaÃ§Ã£o, paralelizaÃ§Ã£o).  
2. **Implementar um pipeline completo**: EDA â†’ PrÃ©-processamento â†’ Treino â†’ AvaliaÃ§Ã£o â†’ InterpretaÃ§Ã£o.  
3. **Comparar desempenho** com o SVM do seminÃ¡rio anterior e discutir vantagens e limitaÃ§Ãµes.  

> âœ¨ *Objetivo principal:* Entender na prÃ¡tica o funcionamento do XGBoost!

---

## ğŸ” 1. Fundamentos TeÃ³ricos

O **XGBoost** (â€œeXtreme Gradient Boostingâ€) Ã© uma tÃ©cnica poderosa baseada em boosting de gradiente, muito utilizada em competiÃ§Ãµes de Machine Learning e na indÃºstria devido ao seu alto desempenho e eficiÃªncia.

> ğŸ‰ Ã‰ uma evoluÃ§Ã£o de algoritmos de boosting tradicionais, trazendo regularizaÃ§Ã£o e paralelizaÃ§Ã£o para o processo.

### 1.1 O que Ã© XGBoost?

- **Boosting** = combinaÃ§Ã£o sequencial de modelos fracos (Ã¡rvores rasas).  
- **Gradient boosting** = cada Ã¡rvore minimiza o gradiente da perda acumulada.  
- **XGBoost** = implementaÃ§Ã£o otimizada com regularizaÃ§Ã£o \(L_1/L_2\) e paralelizaÃ§Ã£o por blocos.

> ğŸ”¹ **Curiosidade:** O XGBoost Ã© considerado "a arma secreta" em competiÃ§Ãµes do Kaggle!

### ğŸ§® 1.2 MatemÃ¡tica Essencial

$$
\begin{aligned}
\mathrm{Obj}(\Theta)
&= \sum_{i=1}^{n} l\bigl(y_i,\hat{y}_i\bigr)
    + \sum_{k=1}^{K}\Omega(f_k),\\
Onde: \\
\Omega(f)
&= \gamma\,T \;+\; \tfrac{1}{2}\,\lambda\,\|w\|^{2}
\end{aligned}
$$

Onde:

- Î˜ = conjunto de parÃ¢metros do modelo
- \(n\) = nÂº de amostras
- \(l\) = funÃ§Ã£o de perda (log-loss neste dataset).
- Î³ = requisito mÃ­nimo de ganho por split
- Å·áµ¢ = prediÃ§Ã£o
- \(K\) = nÂº de Ã¡rvores.
- Î© = funÃ§Ã£o de complexidade (termo de regularizaÃ§Ã£o) aplicado a cada Ã¡rvore  
- \(f_k\) = Ã¡rvore \(k\)
- \(T\) = nÂº de folhas
- \(w\) = pesos das folhas
- Î» = regularizaÃ§Ã£o L2
- Î± = regularizaÃ§Ã£o L1

### 1.3 ParÃ¢metros-chave

| ParÃ¢metro | Efeito | RecomendaÃ§Ãµes iniciais |
|-----------|--------|------------------------|
| `n_estimators` | NÂº de Ã¡rvores | 100â€“300 (+ `early_stopping_rounds`) |
| `learning_rate` | Peso de cada Ã¡rvore | 0.01â€“0.2 (quanto menor, mais Ã¡rvores) |
| `max_depth` | Profundidade | 3â€“7 (maior â†’ +complexo) |
| `subsample` | % de linhas por Ã¡rvore | 0.6â€“1.0 (previne overfitting) |
| `colsample_bytree` | % de colunas por Ã¡rvore | 0.6â€“1.0 |
| `gamma` | MÃ­n. ganho p/ split | 0â€“5 |
| `lambda`, `alpha` | Reg. L2 e L1 | 0â€“10 |

### 1.4 Vantagens Ã— Desvantagens

| ğŸ’ª Vantagens | âš ï¸ Desvantagens |
|--------------|----------------|
| State-of-the-art em dados tabulares; lida com `NaN`. | Muitos hiperparÃ¢metros; tuning pode ser demorado. |
| Treino rÃ¡pido (CPU/GPU) e custo log-loss menor. | Pode sobreajustar se pouco regularizado. |
| Integra regularizaÃ§Ã£o e *early stopping*. | Menos interpretÃ¡vel que modelos lineares. |

### 1.5 Casos de Uso Reais

FinanÃ§as (risco de crÃ©dito), saÃºde (diagnÃ³stico assistido), marketing (churn), detecÃ§Ã£o de fraude, previsÃ£o de demanda.

---

## ğŸ“Š 2. AnÃ¡lise ExploratÃ³ria de Dados (EDA)

```python
import pandas as pd, seaborn as sns, matplotlib.pyplot as plt

# Carregar dataset
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer(as_frame=True)
df = data.frame
df['target'] = data.target

# Visualizar primeiras linhas
df.head()
```

```python
# DistribuiÃ§Ã£o das classes
sns.countplot(x='target', data=df)
plt.title('DistribuiÃ§Ã£o das Classes')
plt.show()

# CorrelaÃ§Ã£o entre variÃ¡veis
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), cmap='coolwarm')
plt.title('Mapa de CorrelaÃ§Ã£o')
plt.show()

df.info()

# Verificar dados faltantes
df.isnull().sum()

# Verificar dados duplicados
df.duplicated().sum()

# Verificar estatÃ­sticas descritivas
df.describe()
```

## ğŸ› ï¸ 3. PrÃ©-processamento

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Remover duplicados
df_clean = df.drop_duplicates()
df_clean.reset_index(drop=True, inplace=True)

# Separar features e target
X = df_clean.drop('target', axis=1)
y = df_clean['target']

# NormalizaÃ§Ã£o
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Dividir treino/teste
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

```

>ğŸ—’ï¸ Por serem Ã¡rvores, a padronizaÃ§Ã£o nÃ£o Ã© obrigatÃ³ria; mantivemos para alinhar com o SVM anterior.

## ğŸš€ 4. Treinamento e Tuning

```python
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
params = {
    'max_depth': [3,5,7],
    'n_estimators': [100,200,300],
    'learning_rate': [0.01,0.05,0.1],
    'subsample': [0.8,1.0],
    'colsample_bytree': [0.8,1.0],
    'gamma': [0,1,5],90
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
model = RandomizedSearchCV(
    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    params, n_iter=20, cv=cv, scoring='roc_auc', random_state=42)
model.fit(X_train, y_train, eval_set=[(X_test, y_test)],
          early_stopping_rounds=20, verbose=False)
best_clf = model.best_estimator_

```

> ğŸ”¹ **Dica:** Teste diferentes hiperparÃ¢metros para melhorar a performance!

## ğŸ“ˆ 5. AvaliaÃ§Ã£o

```python
from sklearn.metrics import classification_report, confusion_matrix, RocCurveDisplay

# PrevisÃ£o
y_pred = model.predict(X_test)

# MÃ©tricas
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# AUC-ROC
roc_display = RocCurveDisplay.from_estimator(model, X_test, y_test)
plt.show()

```

### 5.1 ImportÃ¢ncia das Features

```python
import xgboost as xgb, shap
xgb.plot_importance(best_clf, max_num_features=10)
explainer = shap.TreeExplainer(best_clf)
shap.summary_plot(explainer.shap_values(X_test), X_test, plot_type="bar")

```

## ğŸ§ 6. AnÃ¡lise CrÃ­tica

| QuestÃ£o                | ObservaÃ§Ã£o                                                                              |
| ---------------------- | --------------------------------------------------------------------------------------- |
| **Overfitting**        | Foi mitigado por `early_stopping_rounds` + regularizaÃ§Ã£o?                               |
| **ComparaÃ§Ã£o com SVM** | XGBoost superou AUC-ROC? Vale o custo de complexidade?                                  |
| **Melhorias Futuras**  | Testar *ensemble* stacking SVM + XGBoost; ajustar `scale_pos_weight`; avaliar LightGBM. |

> ğŸ”¹ **ReflexÃ£o:** O XGBoost foi eficaz para este problema? Por quÃª?

## âš™ï¸ Como Reproduzir

```bash
# 1. Ambiente
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Notebook
jupyter lab  # ou VSCode / Google Colab

```

## ğŸš€ Artefatos

| Arquivo              | DescriÃ§Ã£o                                   |
| -------------------- | ------------------------------------------- |
| `xgboost.ipynb`      | Notebook completo com cÃ³digos e grÃ¡ficos    |
| `slides_xgboost.pdf` | Slides de 15 min usados na apresentaÃ§Ã£o     |

## ğŸ“š ReferÃªncias

- ğŸ”—[XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)
- ğŸ”—[Breast Cancer Wisconsin Dataset - Kaggle](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)
- ğŸ”—[XGBoost Docs](https://xgboost.readthedocs.io/)
- ğŸ”—[StatQuest: Gradient Boosting Explained](https://youtu.be/3CC4N4z3GJc)
- ğŸ”—[Kaggle XGBoost Tutorials](https://www.kaggle.com/tag/xgboost)
- ğŸ”—[Scikit-learn Docs](https://scikit-learn.org/stable/index.html)
- ğŸ”—[Bias-Variance Tradeoff](https://scott.fortmann-roe.com/docs/BiasVariance.html)